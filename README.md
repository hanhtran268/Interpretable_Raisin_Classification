# Interpretable_Raisin_Classification

## ğŸ“Œ Project Summary  
This project explored **model interpretability** using the [Raisin Binary Classification dataset](https://www.kaggle.com/datasets/nimapourmoradi/raisin-binary-classification).  
The dataset contains geometric features of raisin images, with the goal of classifying them into two categories.  
The main objective was not only to build predictive models but also to **interpret the decision-making process** behind their predictions.  

## ğŸ”§ Implemented Models  
- ğŸ“Š **Logistic Regression** â€“ captured linear relationships with clear key influencers (MajorAxisLength, ConvexArea, Perimeter).  
- ğŸ¤– **Neural Network** â€“ modeled complex, non-linear interactions with more distributed feature importance (notably MinorAxisLength).  

## ğŸ› ï¸ Interpretability Techniques  
- ğŸ“ˆ **PDP & ICE** â€“ showed feature effects and individual interactions across ranges.  
- âš¡ **ALE** â€“ improved accuracy for correlated features with faster computation.  
- ğŸ“ **Ridge Surrogate** â€“ provided a simplified linear explanation of complex models.  
- ğŸ¯ **SHAP** â€“ delivered detailed local & global feature attributions.  

## âœ… Key Findings  
- Logistic Regression offered **simple and stable feature effects**, easy to explain.  
- Neural Networks captured **richer, non-linear dynamics** but were harder to interpret.  
- SHAP gave the **most granular insights**, though computationally expensive.  
- ALE provided a good balance between **accuracy and interpretability**.  

## ğŸ† Conclusion  
The project highlights the trade-off between **accuracy and interpretability**:  
- Logistic Regression â†’ easier to interpret, stable.  
- Neural Network â†’ more complex, required advanced interpretability tools.  
